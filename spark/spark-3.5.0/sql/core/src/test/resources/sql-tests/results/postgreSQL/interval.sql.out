-- Automatically generated by SQLQueryTestSuite
-- !query
SELECT interval '999' second
-- !query schema
struct<INTERVAL '16 minutes 39 seconds':interval>
-- !query output
16 minutes 39 seconds


-- !query
SELECT interval '999' minute
-- !query schema
struct<INTERVAL '16 hours 39 minutes':interval>
-- !query output
16 hours 39 minutes


-- !query
SELECT interval '999' hour
-- !query schema
struct<INTERVAL '999 hours':interval>
-- !query output
999 hours


-- !query
SELECT interval '999' day
-- !query schema
struct<INTERVAL '999 days':interval>
-- !query output
999 days


-- !query
SELECT interval '999' month
-- !query schema
struct<INTERVAL '83 years 3 months':interval>
-- !query output
83 years 3 months


-- !query
SELECT interval '1' year
-- !query schema
struct<INTERVAL '1 years':interval>
-- !query output
1 years


-- !query
SELECT interval '2' month
-- !query schema
struct<INTERVAL '2 months':interval>
-- !query output
2 months


-- !query
SELECT interval '3' day
-- !query schema
struct<INTERVAL '3 days':interval>
-- !query output
3 days


-- !query
SELECT interval '4' hour
-- !query schema
struct<INTERVAL '4 hours':interval>
-- !query output
4 hours


-- !query
SELECT interval '5' minute
-- !query schema
struct<INTERVAL '5 minutes':interval>
-- !query output
5 minutes


-- !query
SELECT interval '6' second
-- !query schema
struct<INTERVAL '6 seconds':interval>
-- !query output
6 seconds


-- !query
SELECT interval '1-2' year to month
-- !query schema
struct<INTERVAL '1 years 2 months':interval>
-- !query output
1 years 2 months


-- !query
SELECT interval '1 2:03' day to hour
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 36,
    "fragment" : "'1 2:03' day to hour"
  } ]
}


-- !query
SELECT interval '1 2:03:04' day to hour
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 39,
    "fragment" : "'1 2:03:04' day to hour"
  } ]
}


-- !query
SELECT interval '1 2:03' day to minute
-- !query schema
struct<INTERVAL '1 days 2 hours 3 minutes':interval>
-- !query output
1 days 2 hours 3 minutes


-- !query
SELECT interval '1 2:03:04' day to minute
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]d h:m`, `INTERVAL [+|-]'[+|-]d h:m' DAY TO MINUTE` when cast to interval day to minute: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 41,
    "fragment" : "'1 2:03:04' day to minute"
  } ]
}


-- !query
SELECT interval '1 2:03' day to second
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]d h:m:s.n`, `INTERVAL [+|-]'[+|-]d h:m:s.n' DAY TO SECOND` when cast to interval day to second: 1 2:03, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 38,
    "fragment" : "'1 2:03' day to second"
  } ]
}


-- !query
SELECT interval '1 2:03:04' day to second
-- !query schema
struct<INTERVAL '1 days 2 hours 3 minutes 4 seconds':interval>
-- !query output
1 days 2 hours 3 minutes 4 seconds


-- !query
SELECT interval '1 2:03' hour to minute
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]h:m`, `INTERVAL [+|-]'[+|-]h:m' HOUR TO MINUTE` when cast to interval hour to minute: 1 2:03, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 39,
    "fragment" : "'1 2:03' hour to minute"
  } ]
}


-- !query
SELECT interval '1 2:03:04' hour to minute
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]h:m`, `INTERVAL [+|-]'[+|-]h:m' HOUR TO MINUTE` when cast to interval hour to minute: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 42,
    "fragment" : "'1 2:03:04' hour to minute"
  } ]
}


-- !query
SELECT interval '1 2:03' hour to second
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]h:m:s.n`, `INTERVAL [+|-]'[+|-]h:m:s.n' HOUR TO SECOND` when cast to interval hour to second: 1 2:03, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 39,
    "fragment" : "'1 2:03' hour to second"
  } ]
}


-- !query
SELECT interval '1 2:03:04' hour to second
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]h:m:s.n`, `INTERVAL [+|-]'[+|-]h:m:s.n' HOUR TO SECOND` when cast to interval hour to second: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 42,
    "fragment" : "'1 2:03:04' hour to second"
  } ]
}


-- !query
SELECT interval '1 2:03' minute to second
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]m:s.n`, `INTERVAL [+|-]'[+|-]m:s.n' MINUTE TO SECOND` when cast to interval minute to second: 1 2:03, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 41,
    "fragment" : "'1 2:03' minute to second"
  } ]
}


-- !query
SELECT interval '1 2:03:04' minute to second
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_0063",
  "messageParameters" : {
    "msg" : "Interval string does not match day-time format of `[+|-]m:s.n`, `INTERVAL [+|-]'[+|-]m:s.n' MINUTE TO SECOND` when cast to interval minute to second: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0."
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 17,
    "stopIndex" : 44,
    "fragment" : "'1 2:03:04' minute to second"
  } ]
}
